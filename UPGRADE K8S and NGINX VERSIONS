ingress controller version compatibility with Kubernetes version: https://github.com/kubernetes/ingress-nginx

Step-01: Reserving Static Public-IP
az network public-ip create \
  --resource-group <your-resource-group> \
  --name <your-public-ip-name> \
  --sku Standard \
  --allocation-method Static \
  --location <your-region>

az network public-ip create \
  --resource-group MC_RG-AzureProject-dev_AKS-AzureProject-dev_eastus \
  --name nginx-ingress-ip \
  --sku Standard \
  --allocation-method Static \
  --location eastus

NOTE: Make sure to create Static-IP in your AKS RG


Step-02: Add and Update the Ingress-NGINX Helm Repo
curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update

Step-03: Create namespace
kubectl create ns nginx


Step-03: Installing nginx-v1.11.5
helm install new-ingress-nginx ingress-nginx/ingress-nginx \
  --namespace nginx \
  --create-namespace \
  --version 4.10.1 \
  --set controller.image.tag="v1.11.5" \
  --set controller.resources.requests.cpu="25m" \
  --set controller.resources.requests.memory="150Mi" \
  --set controller.resources.limits.cpu="125m" \
  --set controller.resources.limits.memory="200Mi" \
  --set controller.replicaCount=2 \
  --set controller.service.type=LoadBalancer \
  --set controller.service.externalTrafficPolicy=Local \
  --set controller.service.loadBalancerIP="20.121.216.130" \
  --set controller.ingressClass=nginx \
  --set controller.ingressClassResource.name=nginx \
  --set controller.ingressClassResource.enabled=true \
  --set controller.ingressClassResource.default=false

Step-04:Verify Controller is Up
kubectl get pods -n nginx
kubectl get svc -n nginx

---------------------------
1. Jenkins Deployment + Service YAML
jenkins-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: Jenkins
  labels:
    app: jenkins
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jenkins
  template:
    metadata:
      labels:
        app: jenkins
    spec:
      containers:
      - name: jenkins
        image: jenkins/jenkins:lts
        ports:
        - containerPort: 8080
        - containerPort: 50000  # For JNLP agents
        # Optionally add resources, env vars, volumes here
---
apiVersion: v1
kind: Service
metadata:
  name: jenkins
  labels:
    app: jenkins
spec:
  type: ClusterIP
  selector:
    app: jenkins
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP


2. Ingress Resource for Jenkins
How to access Jenkins without a domain name:
Get your Ingress Controller’s external IP: kubectl get svc -n nginx

jenkins-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: jenkins-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/proxy-body-size: "1024m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "60s"
spec:
  ingressClassName: nginx  # <-- Recommended field to specify Ingress controller
  rules:
  - host: jenkins.20.121.216.130.nip.io   # Replace <20.121.216.130> with the external IP
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: jenkins
            port:
              number: 80


kubectl apply -f jenkins-deployment.yaml
kubectl apply -f jenkins-ingress.yaml

NOTE: Make sure to allow port 8080 in NSG.
source: Interet, destination: Public-IP, port: 8080
nip.io is a free wildcard DNS service that automatically maps domain names to IP addresses. It’s extremely helpful in development and testing environments


To access the application use: jenkins.20.121.216.130.nip.io

--------------------------------------
Scenario you're working on:
- you installed ingress version: 1.10.1 which is compatible with k8s version: 1.30
- now you have to upgrade k8s to version: 1.31 where ingress v1.10.1 is not compatible
- so first upgrade ingress controller version to k8s v1.31 compatible and then start upgrading k8s


Upgrading ingress from "v1.10.1" to "v1.12.0"
- Upgrade flow "v1.10.1" to "v1.11.0" and then to "v1.12.0"

Step-01: Get the installed version
- If installed using Helm use : helm list -n nginx
- In other cases describe nginx pods and check image version: kubectl describe po <pod name> -n <namespace>

Step-02: Check the compatible versions
- https://github.com/kubernetes/ingress-nginx
 - Confirm v1.11.0 of the Ingress Controller supports K8s v1.31

Step-03: Pre-Upgrade Checklist
- Review Breaking Changes: https://github.com/kubernetes/ingress-nginx/releases/
  - Go to your preferred version and Look out for: "Deprecated annotations or flags", "Changes in default behavior (timeouts, headers, rewrite behavior)", "Webhook configuration updates"
- Backup Current Configurations:
  - helm get values new-ingress-nginx -n nginx -o yaml > ingress-values-backup.yaml
(This command retrieves the current Helm configuration values (custom values you used during installation or upgrades) for the new-ingress-nginx release in the nginx namespace(like Controller service type (LoadBalancer/ClusterIP), Annotations, Resource limits, Custom arguments, Enabled features (like snippets, metrics, etc.)))

  - kubectl get ingress --all-namespaces -o yaml > all-ingresses-backup.yaml
(This command fetches all Ingress resources across all namespaces in the cluster in full YAML format.)

- Check for Custom Snippets~~~~~
  - You previously hit an error using nginx.ingress.kubernetes.io/configuration-snippet.
  - If your new version still has disable-snippets: true (default from v1.9+), you must explicitly allow snippets if you intend to use them:
[
controller:
  allowSnippetAnnotations: true
]

Step-04: Upgrade Process Using Helm
- Update the Repo: helm repo update
- Preview the Upgrade(Review what will change)
helm plugin install https://github.com/databus23/helm-diff
helm diff upgrade new-ingress-nginx ingress-nginx/ingress-nginx \
  --namespace nginx \
  --version 4.11.0 \
  -f ingress-values-backup.yaml

- Perform the Upgrade
helm upgrade new-ingress-nginx ingress-nginx/ingress-nginx \
  --namespace nginx \
  --version 4.11.0 \
  -f ingress-values-backup.yaml

Step-05: Post-Upgrade Verification
- Verify Pods: kubectl get pods -n nginx
- Check Version: helm list -n nginx
- Check Logs for Errors: kubectl logs -n nginx -l app.kubernetes.io/name=ingress-nginx

Step-06: Rollback Plan(Optional)
- If something breaks: helm rollback new-ingress-nginx <previous_revision_number> -n nginx
- You can get the revision number via: helm history new-ingress-nginx -n nginx

NOTE: Repeat the process to upgrade from v1.11.0 to v1.12.0
----------------------------
Now start upgrading your k8s cluster:

Events that happen when you upgrade your cluster
- Surge: New surge node will be creates
- Drain: Pods are evicted from the node. Each pod has a 5 minute timeout to complete the eviction.
- Upgrade: Update of a node has succeeded or failed
- Delete: Delete surge node.
NOTE: Your can only upgrade one minor version at a time.


Step-01: Finding what versions are available for your subscription & region
- az aks get-versions --location eastus --output table

Step-02: Check current version
- az aks get-upgrades --resource-group <rg-name> --name <cluster-name> --output table

az aks get-upgrades --resource-group RG-AzureProject-dev --name AKS-AzureProject-dev --output table

Step-03: Backup all resources yaml
sudo vim get-all-services-yaml.sh

#!/bin/bash

# Output base directory
OUTPUT_DIR="./aks-services-backup"

# Create output directory
mkdir -p "$OUTPUT_DIR"

# Get all namespaces
namespaces=$(kubectl get namespaces -o jsonpath='{.items[*].metadata.name}')

# Loop through each namespace
for ns in $namespaces; do
  echo "Processing namespace: $ns"
  mkdir -p "$OUTPUT_DIR/$ns"

  # Get all services in the namespace
  services=$(kubectl get svc -n "$ns" -o jsonpath='{.items[*].metadata.name}')
  
  for svc in $services; do
    echo "  Exporting service: $svc"
    kubectl get svc "$svc" -n "$ns" -o yaml > "$OUTPUT_DIR/$ns/$svc.yaml"
  done
done

echo "✅ Backup complete. YAML files saved in: $OUTPUT_DIR"

sudo chmod +x get-all-services-yaml.sh
sh get-all-services-yaml.sh

--------------------------------------------
Keep this step on hold
--------------------------------------------
Step-04: Pre-upgrade plan
- Identify the breaking changes in existing version using "datree"
If you use above script to backup your services yaml then a file name "aks-services-backup" will be created. Go into each folder and execute: datree test --schema-version "1.31.1" *.yaml
NOTE: Run this in all yaml containing folders.
datree: To identify the breaking changes
- To validate the schema version & mis-configuration in your k8s.
  - Get all your service.yaml files & then validate
  - In your yaml containing folder run: datree test --schema-version "k8s version to upgrade" *.yaml
NOTE: Ignore all errors and focus on schema validation.
--------------------------------------------
Start Upgrading: 
Azure portal--AKS cluster--Settings--Upgrades--k8s version(upgrade version)--select the version you want to move to--first upgrade only control plane--save
Once control plane is upgraded Start upgrading nodepool:
Azure portal--AKS cluster--Settings--nodepool--select your nodepool--upgrade k8s--select version you want to upgrade to--select


--------------------------------------------
Keep this step on hold
--------------------------------------------
Step-05: Check events
- watch kubectl.exe get events (or)
- kubectl.exe get events
--------------------------------------------
NOTE: You can see a surge node created.
