AKS with UserNodePool-2

Step-1: HELM INSTALLATION
=========================
Install Helm: curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
Verify Installation: helm version

Step-2: CREATE NAMESPACE
========================
kubectl create ns logging

USEFUL:
To determine the number of volumes that can be allocated per node: kubectl get CSINode <nodename> -o yaml


Step-3: CREATE STORAGCLASS
==========================
---
vim sc.yaml
---
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: custom-csi
provisioner: disk.csi.azure.com
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true

kubectl create -f sc.yaml


Step-4: LBELLING NODES
======================
kubectl label nodes <node name> agentpool=userpool


Step-5: DEPLOY ELASTICSEARCH
============================
helm repo add elastic https://helm.elastic.co
helm repo update

---
vim sc-values.yaml
---
replicas: 2  # Increase to match 2 nodes

volumeClaimTemplate:
  accessModes: ["ReadWriteOnce"]  # Change if your CSI driver supports it
  resources:
    requests:
      storage: 30Gi  # Increase if your index grows
  storageClassName: "custom-csi"

nodeSelector:
  agentpool: "usernodepool"

resources:
  requests:
    cpu: "1"
    memory: "4Gi"
  limits:
    cpu: "2"
    memory: "8Gi"


helm install elasticsearch elastic/elasticsearch -f sc-values.yaml --namespace logging



Step-6: RETRIEVE ELASTICSEARCH UNAME AND PASSWORD
=================================================
# for username
kubectl get secrets --namespace=logging elasticsearch-master-credentials -ojsonpath='{.data.username}' | base64 -d
# for password
kubectl get secrets --namespace=logging elasticsearch-master-credentials -ojsonpath='{.data.password}' | base64 -d


username: elastic
passwd: m7LV3bu8mXJvcb4j


Step-7: DEPLOY KIBANA
======================
helm install kibana --set service.type=LoadBalancer elastic/kibana -n logging


Step-8: ACCESS KIBANA
=====================
kubectl get  svc -n logging
(get kibana svc External-IP and access it in browser)

EXPERNAL-IP:5601
(Login to it using credentials that are generated earlier)


Step-9: MODIFY USERNAME AND PASSWORD IN "fluentbit-values.yaml"
=============================================================== 
Get it from: https://github.com/summu97/Azure-KT/blob/main/EFK/fluentbit-values.yaml
(Copy to your local)
Change lines 442, 443, 457, 458 with the credentials you got in Step-6


Step-10: INSTALL FLUENTBIT
==========================
helm repo add fluent https://fluent.github.io/helm-charts
helm install fluent-bit fluent/fluent-bit -f fluentbit-values.yaml -n logging

Note: As fluentbit runs as Daemonset you'll get each pod for each node



========================
TO UNINSTALL EVERYTHING:
========================
kubectl run -n logging debug --rm -it --image=alpine -- /bin/sh

To Uninstall:
helm uninstall elasticsearch -n logging
helm uninstall kibana -n logging
helm uninstall fluent-bit -n logging

kubectl delete pvc --all -n logging
kubectl delete pv <pv-name>


kubectl get all -n logging
kubectl get pvc -n logging
kubectl get pv
kubectl get ns

==========
DEBUGGING:
==========
kubectl run --rm -it debug --image=alpine -- sh
# Inside the Alpine shell:
apk add curl
curl -XGET "http://elasticsearch-master.logging.svc.cluster.local:9200"




-------------------------------------------
What Are Shards in Elasticsearch?
Shards are subdivisions of an index in Elasticsearch. Each index is split into multiple shards to allow parallel processing, faster searches, and scalability.


