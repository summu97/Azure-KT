AKS with UserNodePool-2

Step-1: HELM INSTALLATION
=========================
Install Helm: curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
Verify Installation: helm version

Step-2: CREATE NAMESPACE
========================
kubectl create ns logging

USEFUL:
To determine the number of volumes that can be allocated per node: kubectl get CSINode <nodename> -o yaml


Step-3: CREATE STORAGECLASS
==========================
---
vim sc.yaml
---
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: custom-csi
provisioner: disk.csi.azure.com
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true

kubectl create -f sc.yaml


Step-4: LABELLING NODES
======================
kubectl label nodes <node name> agentpool=userpool

To check labels: kubectl get nodes -o jsonpath="{range .items[*]}{.metadata.name} -> {.metadata.labels}{'\n'}{end}"


Step-5: DEPLOY ELASTICSEARCH
============================
helm repo add elastic https://helm.elastic.co
helm repo update

---
vim sc-values.yaml
---
replicas: 2  # Increase to match 2 nodes

volumeClaimTemplate:
  accessModes: ["ReadWriteOnce"]  # Change if your CSI driver supports it
  resources:
    requests:
      storage: 30Gi  # Increase if your index grows
  storageClassName: "custom-csi"

nodeSelector:
  agentpool: "usernodepool"

resources:
  requests:
    cpu: "1"
    memory: "4Gi"
  limits:
    cpu: "2"
    memory: "8Gi"


helm install elasticsearch elastic/elasticsearch -f sc-values.yaml --namespace logging



Step-6: RETRIEVE ELASTICSEARCH UNAME AND PASSWORD
=================================================
# for username
kubectl get secrets --namespace=logging elasticsearch-master-credentials -ojsonpath='{.data.username}' | base64 -d
# for password
kubectl get secrets --namespace=logging elasticsearch-master-credentials -ojsonpath='{.data.password}' | base64 -d


username: elastic
passwd: m7LV3bu8mXJvcb4j


Step-7: DEPLOY KIBANA
======================
helm install kibana --set service.type=LoadBalancer elastic/kibana -n logging


Step-8: ACCESS KIBANA
=====================
kubectl get  svc -n logging
(get kibana svc External-IP and access it in browser)

EXPERNAL-IP:5601
(Login to it using credentials that are generated earlier)

Step-9: CREATE CUSTOM ADMIN USER
================================
login to kibana--click on menu tab--management--security
ROLES: create role--role name--cluster priviledges(all)--run as priviledges(*)--(indices(*)--priviledges(all))--Add index priviledge(to add one more index)--(indices(*)--priviledges(monitor, read, view_index_metadata, read_cross_cluster))--create role
USERS: Crete user--go through options--roles(select the custom role you created)



Step-10 : GENERETE SECRETS WITH USERNAME AND PASSWORD OF YOUR CUSTOM ADMIN USER
==============================================================================
File link: https://github.com/summu97/Azure-KT/blob/main/EFK/fluentbit-values.yaml

kubectl create secret generic elasticsearch-credentials --from-literal=ELASTICSEARCH_USERNAME='<username>'  --from-literal=ELASTICSEARCH_PASSWORD='<password>' --namespace=logging

Note: You need to reference these secrets in fluentbit-values.yaml file(line- 312, 318) only, if you change the name is secrets(if not changed anything then dont touch it).

To check content of passwords: kubectl get secret elasticsearch-credentials -n logging -o jsonpath='{.data}' | jq -r 'to_entries | .[] | "\(.key): \(.value | @base64d)"'


Step-11: INSTALL FLUENTBIT
==========================
helm repo add fluent https://fluent.github.io/helm-charts
helm install fluent-bit fluent/fluent-bit -f fluentbit-values.yaml -n logging

Note: As fluentbit runs as Daemonset you'll get each pod for each node


Useful when any changes are done in fluentbit-values file and want to apply them: helm install fluent-bit fluent/fluent-bit -f fluentbit-values.yaml -n logging

========================
TO UNINSTALL EVERYTHING:
========================
kubectl run -n logging debug --rm -it --image=alpine -- /bin/sh

To Uninstall:
helm uninstall elasticsearch -n logging
helm uninstall kibana -n logging
helm uninstall fluent-bit -n logging

kubectl delete pvc --all -n logging
kubectl delete pv <pv-name>


kubectl get all -n logging
kubectl get pvc -n logging
kubectl get pv
kubectl get ns

==========
DEBUGGING:
==========
kubectl run --rm -it debug --image=alpine -- sh
# Inside the Alpine shell:
apk add curl
curl -XGET "http://elasticsearch-master.logging.svc.cluster.local:9200"




-------------------------------------------
What Are Shards in Elasticsearch?
Shards are subdivisions of an index in Elasticsearch. Each index is split into multiple shards to allow parallel processing, faster searches, and scalability.
=============================================================================
TO DELETE KIBANA MANUALLY:

helm uninstall kibana -n logging --no-hooks
kubectl delete configmap kibana-kibana-helm-scripts -n logging
kubectl delete all -l app=kibana -n logging
kubectl delete configmap -l app=kibana -n logging
kubectl delete secret -l app=kibana -n logging
kubectl delete serviceaccount -l app=kibana -n logging
kubectl delete clusterrolebinding -l app=kibana
kubectl delete rolebinding pre-install-kibana-kibana -n logging
kubectl delete all --selector app=kibana -n logging
kubectl delete jobs --selector release=kibana -n logging
kubectl delete pvc --selector app=kibana -n logging
kubectl delete role pre-install-kibana-kibana -n logging
kubectl delete serviceaccount pre-install-kibana-kibana -n logging
kubectl delete secret kibana-kibana-es-token -n logging


helm list -n logging


verify: kubectl get all -n logging | grep kibana
=============================================================================
